{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BA-LR: Binary-Attribute based Likelihood Ratio estimation for explainable speaker recognition","text":"<p>This library contains tools which can be used for explainable deep speaker recognition and modeling based on voice attributes. It is based on the doctorate work of Imen Ben Amor but offers improved models and utilities for binary-attribute based speaker modeling.</p> <p>The first step in the BA-LR approach for speaker recognition is the extraction of binary-attribute based speech representations. This can be done either with a model that works with raw audio files, or with a model pre-trained on speaker embeddings extracted using an embedding model, such as wespeaker.</p>"},{"location":"binary_attributes/","title":"Binary-Attribute representation extraction","text":"<p>The BA-LR toolkit provides an <code>AutoEncoder</code> model architecture to extract binary-attribute representations. The <code>BinaryAttributeAutoEncoder</code> class must use a trained model checkpoint to perform inference of binary-attribute representations. The training of the <code>BinaryAttributeAutoEncoder</code> can be done with the trainer module provided by the BA-LR toolkit.</p>"},{"location":"binary_attributes/#binaryattributeautoencoder","title":"BinaryAttributeAutoEncoder","text":"<pre><code>class BinaryAttributeAutoEncoder(BinaryAttributeEncoder):\n    def __init__(\n        self,\n        checkpoint_path: str | Path,\n        input_dim: int = 256,\n        internal_dim: int = 512,\n        device: str | torch.device = \"cpu\",\n    ):\n</code></pre> <p>Parameters:</p> <ul> <li>checkpoint_path: path to a trained model checkpoint. By default, uses the one provided in <code>resources/models/BAE/BAE_mse.pt</code>.</li> <li>input_dim: the input dimension for the AutoEncoder (i.e. the embedding dimension).</li> <li>internal_dim: the internal or output dimension for the AutoEncoder (i.e. the dimension of the binary-attribute representation).</li> <li>device: the device to use the model on.</li> </ul>"},{"location":"binary_attributes/#extract-binary-attribute-representations","title":"Extract binary-attribute representations","text":"<pre><code>def get_binary_attributes(\n    self, dataset: AudioDataset, stream_save: bool, data_config: DataConfig\n) -&gt; list[tuple[str, npt.NDArray]]:\n</code></pre> <p>Parameters:</p> <ul> <li>dataset: the dataset to extract binary-attribute vectors for.</li> <li>stream_save: if True, save the binary-attribute vectors to disk as they are computed, returning an empty list. Otherwise, return a list of id and binary-attribute vector  tuples.</li> <li>data_config: config parameters for the <code>Dataloader</code> used with the dataset.</li> </ul> <p>The <code>get_binary_attributes</code> method can be called on the <code>BinaryAttributeAutoEncoder</code> class to extract binary-attribute representations for the embedding files in the given dataset. Elements in the dataset without an <code>embedding</code> vector will be skipped.</p>"},{"location":"binary_attributes/#cli","title":"CLI","text":"<p>The BA-LR cli provides a <code>binarize</code> command to extract binary-attribute representations from embedding vectors in an <code>AudioDataset</code>.</p> <p>Parameters:</p> <ul> <li>input: path to the dataset to process.</li> <li>force: whether to force extraction of samples that already have binary-attribute vectors.</li> <li>save_output: whether to save binary-attribute vectors to disk as they are extracted.</li> <li>save_dir: directory where the binary-attribute vectors will be saved (sets the <code>output_dir</code> parameter on the <code>AudioDataset</code>).</li> <li>audio_formats: optional list of audio file extensions to load if <code>input</code> points to a directory of audio files.</li> <li>device: the device to use for binarization.</li> <li>overrides: optional hydra config overrides.</li> </ul> <p>Example</p> <p><pre><code>balr binarize resources/data/voxceleb2/metadata.csv\n</code></pre> will extract binary-attribute vectors for all the embedding vectors present in the <code>voxceleb2</code> dataset.</p> <p>Example</p> <pre><code>balr binarize --force --save-dir resources/data/voxceleb2-ba --device cuda resources/data/voxceleb2/metadata.csv encoder.model.checkpoint_path=runs/train/best.pt\n</code></pre> <p>will extract binary-attribute vectors for all the embedding vectors present in the <code>voxceleb2</code> dataset, even those which already have binary-attribute vectors (<code>--force</code>), saving these vectors to a different directory (<code>--save-dir resources/data/voxceleb2-ba</code>), using a gpu for binarization (<code>--device cuda</code>) and using a custom trained model checkpoint for the <code>AutoEncoder</code> (<code>encoder.model.checkpoint_path=runs/train/best.pt</code>).</p>"},{"location":"datasets/","title":"Datasets","text":"<p>The BA-LR toolkit provides a custom dataset class (which extends the <code>torch.utils.data.Dataset</code> class) to manage audio files and their vector representations.</p>"},{"location":"datasets/#audiodataset","title":"AudioDataset","text":"<p>The <code>AudioDataset</code> represents a dataset of <code>AnnotatedAudio</code> items. Each item in the dataset is identified by an id and provides information for an audio file, its labels (speaker id) and optional computed attributes, such as an embedding vector, or binary attribute vectors.</p> <p>The dataset takes as input a dict of dicts as the collection of data points to read. The top level keys are data point IDs. Each data point dict should have the same keys, corresponding to different files in that data point.</p> <p>For example the input data could look like this</p> <pre><code>data = {\n    \"spk1utt1\": {\n        \"audio\": \"/path/to/spk1utt1.wav\",\n        \"embedding\": \"/path/to/spk1utt1_emb.txt\",\n        \"binary_attributes\": \"/path/to/spk1utt1_ba.txt\",\n    },\n    \"spk1utt2\": {\n        \"wav_file\": \"/path/to/spk1utt2.wav\",\n        \"embedding\": \"/path/to/spk1utt2_emb.txt\",\n        \"binary_attributes\": \"/path/to/spk1utt2_ba.txt\",\n    }\n}\n</code></pre> <p>Note</p> <p>The files are only loaded in memory when the element they belong to is accessed.</p>"},{"location":"datasets/#annotatedaudio","title":"AnnotatedAudio","text":"<p>Elements of an <code>AudioDataset</code> can be accessed by their index. They are returned as an <code>AnnotatedAudio</code> object containing the element's id, label, and tensors corresponding to the audio signal, and embedding or binary attribute representations.</p> <pre><code>class AnnotatedAudio:\n    id: str\n    sample_rate: int\n    audio_path: Path | None = None\n    audio: torch.Tensor | None = None\n    embedding: torch.Tensor | None = None\n    binary_attributes: torch.Tensor | None = None\n    speaker: str | int | None = None\n</code></pre> <p>Warning</p> <p>At least one of <code>audio</code>, <code>embedding</code> or <code>binary_attributes</code> must be provided for each element in an <code>AudioDataset</code>.</p>"},{"location":"datasets/#audiodatasetfrom_path","title":"AudioDataset.from_path","text":"<p>The <code>AudioDataset</code> class offers a convenience method to create a dataset from a file, a directory of files, or a csv file (containing metadata information on the files in the dataset).</p> <pre><code>@classmethod\ndef from_path(\n    cls,\n    path: str | Path,\n    audio_formats: list[str] = [\"flac\", \"mp3\", \"m4a\", \"ogg\", \"opus\", \"wav\", \"wma\"],\n    sample_rate: int = 16_000,\n) -&gt; AudioDataset:\n</code></pre> <p>Parameters:</p> <ul> <li>path: a path pointing to a file or directory.</li> <li>audio_formats: audio file extensions to look for if <code>path</code> is a directory.</li> <li>sample_rate: the target sample rate at which audio files will be loaded or resampled to if needed.</li> </ul> <p>Warning</p> <p>If <code>path</code> points to a csv file, the csv file must contain at least one column pointing to <code>audio</code>, <code>embedding</code> or <code>binary_attributes</code> file paths, plus other optional columns such as speaker or id. If the id column is not specified, the name of the audio files will be used as ids instead (the <code>audio</code> column must be present).</p> <p>For example, if the csv file looks like this</p> <pre><code>audio,speaker\nid07417/00028.wav,id07417\nid03184/00022.wav,id03184\nid03184/00053.wav,id03184\nid04961/00169.wav,id04961\nid04961/00289.wav,id04961\nid01184/00133.wav,id01184\nid06261/00159.wav,id06261\nid06261/00233.wav,id06261\nid06261/00190.wav,id06261\nid07531/00142.wav,id07531\n</code></pre> <p>Calling <code>AudioDataset.from_path</code> with the path of this csv file will return a Dataset of 10 samples, each sample having an audio file and being labeled with the speaker id from the csv file.</p>"},{"location":"datasets/#vector-files","title":"Vector files","text":"<p>By default, the <code>AudioDataset</code> class will try to look for and load two types of vector files for each sample in the dataset:</p> <ul> <li>embedding files with the suffix <code>_emb</code></li> <li>binary attribute files with the suffix <code>_ba</code></li> </ul> <p>Vector files should have the same name and path as the audio file to which they correspond, with the correct suffix appended. For exemple, if the directory structure looks like</p> <pre><code>spk1utt1.wav        # The speaker audio file.\nspk1utt1_emb.txt    # The speaker embedding vector\nspk1utt1_ba.txt     # The speaker binary-attribute vector\n</code></pre> <p>Then the <code>AudioDataset</code> will automatically load and associate the embedding and binary-attribute files to the audio file with the same name.</p> <p>The <code>AudioDataset</code> class supports loading vector files in numpy text format (with <code>.txt</code> extension) or as pickled numpy arrays (with <code>.pkl</code> extension).</p>"},{"location":"datasets/#loading-a-dataset-of-vector-files-only","title":"Loading a dataset of vector files only","text":"<p>In some cases, you might want to create a dataset of only vector files (embeddings or binary-attributes) without a reference to the audio file itself. This can be done by creating a csv file with a column pointing to the path of the vector file for each sample in the dataset. When no <code>audio</code> column is provided, an <code>id</code> column is required for each sample as well as either an <code>embedding</code> or a <code>binary_attributes</code> column.</p> <p>For example, with a csv file like</p> <pre><code>id,binary_attributes,speaker\nid07417/00028,id07417/00028_ba.txt,id07417\nid03184/00022,id03184/00022_ba.txt,id03184\nid03184/00053,id03184/00053_ba.txt,id03184\nid04961/00169,id04961/00169_ba.txt,id04961\nid04961/00289,id04961/00289_ba.txt,id04961\nid01184/00133,id01184/00133_ba.txt,id01184\nid06261/00159,id06261/00159_ba.txt,id06261\nid06261/00233,id06261/00233_ba.txt,id06261\nid06261/00190,id06261/00190_ba.txt,id06261\nid07531/00142,id07531/00142_ba.txt,id07531\n</code></pre> <p>Calling <code>AudioDataset.from_path</code> with the path of this csv file will return a Dataset of 10 samples, each sample having an id, speaker, and binary_attributes vector.</p>"},{"location":"datasets/#saving-vector-files","title":"Saving vector files","text":"<p>The <code>save_vectors</code> method on an <code>AudioDataset</code> allows saving vectors for elements of the dataset. It will both save the given vectors to disk, and add them to the corresponding elements of the dataset.</p> <pre><code>def save_vectors(self, vectors: list[tuple[str, npt.NDArray]], vector_type: VectorType):\n    \"\"\"\n    Save a list of vectors (embedding, binary attributes) to disk and add\n    them to this dataset.\n    \"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li>vectors: a list of (id, vector) tuples to save.</li> <li>vector_type: the type of vector, either <code>embedding</code> or <code>binary_attributes</code>.</li> </ul> <p>Tip</p> <p>By default, the vector files will be saved in the same directory as the audio file for the given element. If you want to save the vector files in a different directory, for instance if you do not have the permissions to write to the directory where the audio files are located, you can set the <code>output_dir</code> attribute on the <code>AudioDataset</code> with the <code>AudioDataset.set_output_dir</code> method. If <code>output_dir</code> is set on the <code>AudioDataset</code>, calling <code>AudioDataset.save_vectors</code> will instead save the vector files in the <code>output_dir</code> with the same directory structure as the audio files.</p>"},{"location":"embeddings/","title":"Embedding extraction","text":"<p>Extraction of speaker embeddings is the first step towards producing binary-attribute based representations. The BA-LR toolkit provides a wrapper around wespeaker's embedding extraction model.</p>"},{"location":"embeddings/#wespeakermodel","title":"WespeakerModel","text":"<pre><code>class WespeakerModel(EmbeddingsModel):\n    def __init__(\n        self,\n        model_repo: str = \"Wespeaker/wespeaker-voxceleb-resnet34-LM\",\n        model_name: str = \"avg_model\",\n        config_name: str = \"config.yaml\",\n        model_dir: str | Path | None = None,\n        device: str | torch.device = \"cpu\",\n        features: FeaturesConfig = FeaturesConfig(),\n    ):\n</code></pre> <p>Parameters:</p> <ul> <li>model_repo: the name of the Wespeaker model repository on huggingface to load the model from.</li> <li>model_name: the model weights file name within the repository (should usually be <code>avg_model.pt</code> or <code>avg_model</code>).</li> <li>config_name: the model config file name within the repository (should usually be <code>config.yaml</code>).</li> <li>model_dir: the local folder path where model files will be downloaded to. If None, the default will be <code>~/.cache</code>.</li> <li>device: the device to use the model on.</li> <li>features: config parameters for wespeaker's features extraction.</li> </ul>"},{"location":"embeddings/#extract-embeddings","title":"Extract embeddings","text":"<pre><code>def extract_embeddings(\n    self, dataset: AudioDataset, stream_save: bool, data_config: DataConfig\n) -&gt; list[tuple[str, npt.NDArray]]:\n</code></pre> <p>Parameters:</p> <ul> <li>dataset: the dataset to extract embeddings for.</li> <li>stream_save: if True, save the embeddings to disk as they are computed, returning an empty list. Otherwise, return a list of embedding and id tuples.</li> <li>data_config: config parameters for the <code>Dataloader</code> used with the dataset.</li> </ul> <p>The <code>extract_embeddings</code> method can be called on the <code>WespeakerModel</code> class to extract embeddings for the audio files in the given dataset. Elements in the dataset without an <code>audio</code> waveform will be skipped.</p>"},{"location":"embeddings/#cli","title":"CLI","text":"<p>The BA-LR cli provides an <code>extract</code> command to extract embeddings for audio files in an <code>AudioDataset</code>.</p> <p>Parameters:</p> <ul> <li>input: path to the dataset to process.</li> <li>force: whether to force extraction of samples that already have embeddings.</li> <li>save_output: whether to save embeddings to disk as they are extracted.</li> <li>save_dir: directory where the embeddings will be saved (sets the <code>output_dir</code> parameter on the <code>AudioDataset</code>).</li> <li>audio_formats: optional list of audio file extensions to load if <code>input</code> points to a directory of audio files.</li> <li>device: the device to use for embedding extraction.</li> <li>overrides: optional hydra config overrides.</li> </ul> <p>Example</p> <p><pre><code>balr extract resources/data/voxceleb2/metadata.csv\n</code></pre> will extract embedding vectors for all the audio files in the <code>voxceleb2</code> dataset.</p> <p>Example</p> <pre><code>balr extract --force --save-dir resources/data/voxceleb2-emb --device cuda resources/data/voxceleb2/metadata.csv embeddings.model.model_repo=Wespeaker/wespeaker-voxceleb-resnet293-LM embeddings.model.model_name=avg_model.pt\n</code></pre> <p>will extract embedding vectors for all the audio files in the <code>voxceleb2</code> dataset, even those which already have embeddings (<code>--force</code>), saving those embeddings to a different directory (<code>--save-dir resources/data/voxceleb2-emb</code>), using a gpu for extraction (<code>--device cuda</code>) and using the <code>wespeaker-voxceleb-resnet34-LM</code> model.</p>"},{"location":"losses/","title":"Loss functions","text":"<p>The BA-LR toolkit provides some loss functions to use during training of a Binary Attribute Encoder.</p>"},{"location":"losses/#baseautoencoderloss","title":"BaseAutoEncoderLoss","text":"<p>All loss functions which can be used with a <code>BinaryAttributeEncoderTrainer</code> must be subclasses of <code>BaseAutoEncoderLoss</code>.</p> <pre><code>class BaseAutoEncoderLoss(torch.nn.Module, ABC):\n\n    def __init__(self, name: str, weight: torch.Tensor | None = None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        ...\n</code></pre> <p>A <code>BaseAutoEncoderLoss</code> function can have a <code>weight</code> parameter which will be applied to itself to compute the <code>total loss</code> during training (the <code>total loss</code> is the weighted sum of the losses computed by the loss functions of the trainer).</p> <p>All <code>BaseAutoEncoderLoss</code> classes must implement the <code>forward</code> method which computes the loss from the parameters.</p> <pre><code>def forward(\n    self, input: torch.Tensor, labels: torch.Tensor, output: torch.Tensor, recon: torch.Tensor, Z: torch.Tensor,\n) -&gt; torch.Tensor:\n</code></pre> <p>Parameters:</p> <ul> <li>input: input features fed to the model (i.e. embeddings). A torch.Tensor of size (batch_size, embedding_dim).</li> <li>labels: labels associated to the input. A torch.Tensor of size (batch_size).</li> <li>output: model's encoded reprensentation of the input. A torch.Tensor of size (batch_size, encoder_dim).</li> <li>recon: the reconstructed input from the model. A torch.Tensor of size (batch_size, embedding_dim).</li> <li>Z: the model's latent space representation of the input. A torch.Tensor of size (batch_size, encoder_dim).</li> </ul>"},{"location":"losses/#mse-loss","title":"MSE Loss","text":"<p>An implementation of <code>torch.nn.MSELoss</code>.</p>"},{"location":"losses/#triplet-margin-loss","title":"Triplet Margin Loss","text":"<pre><code>class TripletMarginLoss(BaseAutoEncoderLoss):\n\n    def __init__(\n        self, margin: float = 0.3, type_of_triplets: str = \"all\", weight: torch.Tensor | None = None,\n    ):\n</code></pre> <p>Parameters:</p> <ul> <li>margin: the difference between the anchor-positive distance and the anchor-negative distance.</li> <li>type_of_triplets:<ul> <li>\"all\" selects all triplets that violate the margin</li> <li>\"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive</li> <li>\"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive</li> <li>\"easy\" selects all triplets that do not violate the margin.</li> </ul> </li> </ul> <p>An implementation of <code>Triplet Margin Loss</code>. The <code>TripletMarginLoss</code> uses a <code>Triplet Margin Miner</code> to compute all possible triplets within the batch based on the labels. Anchor-positive pairs are formed by embeddings that share the same label, and anchor-negative pairs are formed by embeddings that have different labels. The miner selects positive and negative pairs that are particularly difficult, i.e. all triplets that violate the <code>margin</code> param (the difference between the anchor-positive distance and the anchor-negative distance).</p> <p>Note</p> <p>In order to be able to select triplets, each batch must contain at least 2 distinct classes (to form the anchor-negative pairs) and at least 2 samples per class (to form the anchor-positive pairs). Shuffling the classes grouped together in a batch is also recommended to avoid overfitting to some pairs.</p>"},{"location":"losses/#arcface-loss","title":"ArcFace Loss","text":"<pre><code>class ArcFaceLoss(BaseAutoEncoderLoss):\n\n    def __init__(\n        self, margin: float = 0.3, type_of_triplets: str = \"all\", weight: torch.Tensor | None = None,\n    ):\n        ...\n\n    def setup(self, nb_train_classes: int, internal_dim: int):\n</code></pre> <p>Parameters:</p> <ul> <li>margin: the difference between the anchor-positive distance and the anchor-negative distance.</li> <li>type_of_triplets:<ul> <li>\"all\" selects all triplets that violate the margin</li> <li>\"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive</li> <li>\"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive</li> <li>\"easy\" selects all triplets that do not violate the margin.</li> </ul> </li> <li>nb_train_classes: the number of unique classes in the train dataset.</li> <li>internal_dim: the encoder's dimension.</li> </ul> <p>An implementation of <code>ArcFace Loss</code>. The <code>ArcFaceLoss</code> uses the same <code>Triplet Margin Miner</code> as the <code>TripletMarginLoss</code> to select triplets.</p> <p>The loss's <code>setup</code> method must be called once the trainer is instantiated to provide additionnal parameters. The trainer will call <code>setup</code> on each loss function with named parameters that can be used to setup the loss.</p> <p>Note</p> <p>This loss's parameters must be passed to the optimizer during training.</p>"},{"location":"losses/#sparsity-loss","title":"Sparsity Loss","text":"<p>The goal of the sparsity loss is to push the model to encode binary representations modeled by shared discriminant attributes between speakers. An attribute is considered present in the profile, if the sum of all utterance activations per attribute for a speaker is non-zero, while the typicality is the presence frequency of an attribute amon speaker profiles. So we can regulate the activations of the latent space dimensions before binarization to ensure that only a subset of speakers has a particular dimension present in their profile.</p> <p>The sparsity loss drives the binary-attribute dimensions towards achievin a desired presence frequency among speakers. Mainly, to ensure the absence of an attribute in a speaker's profile, we need to drive the sum of activations across all their utterance vectors to 0. The sparsity loss pushes each dimension to follow a specific sparsity while considering speakers.</p> <p>Note</p> <p>The sparsity loss requires each batch to contain N speakers and M samples per speaker.</p> <p>The sparsity loss works with the latent space representations of the speaker embeddings from the encoder (Tanh activations ranging from -1 to 1. <code>Z</code> in the figure above). The vector <code>V</code> representing the desired frequency presence of attributes is generated randomly when the loss is initialized.</p> <pre><code>class SparsityLoss(BaseAutoEncoderLoss):\n\n    def __init__(self, weight: torch.Tensor | None = None):\n        ...\n\n    def setup(self, M_samples_per_class: int, internal_dim: int):\n</code></pre> <p>The <code>SparsityLoss</code> class does not take parameters at initialization (other than its <code>weight</code>) but its <code>setup</code> method must be called once the trainer is instantiated. The trainer will call <code>setup</code> on each loss function with named parameters that can be used to setup the loss.</p> <p>Parameters:</p> <ul> <li>M_samples_per_class: the number of samples per speaker in each batch.</li> <li>internal_dim: the encoder's dimension.</li> </ul>"},{"location":"losses/#cli","title":"CLI","text":"<p>The losses used during training can be set by overriding the <code>trainer.losses</code> parameter.</p> <p>Example</p> <pre><code>balr train resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv 'trainer.losses=[mse, arcface]'\n</code></pre> <p>will run training with two loss functions: MSE and ArcFace. The <code>total loss</code> during training will be the weighted sum of these two losses.</p> <p>Example</p> <pre><code>balr train resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv 'trainer.losses=[mse, arcface]' trainer.losses.arcface.weight=1 trainer.losses.arcface.margin=0.5\n</code></pre> <p>this example also modifies the parameters for the ArcFace loss, changing its weight and margin parameters.</p>"},{"location":"samplers/","title":"Samplers","text":"<p>The BA-LR toolkit provides <code>torch.utils.data.Samplers</code> to enforce certain conditions on the batches used for training a Binary Attribute Encoder.</p>"},{"location":"samplers/#nxmsampler","title":"NxMSampler","text":"<p>A <code>NxMSampler</code> samples a dataset in batches where each batch of size N * M contains N classes and M samples per class. Different implementations of the NxMSampler can enforce various extra conditions, such as ensuring that each batch contains N distinct classes.</p> <p>Warning</p> <p>A dataset used with a <code>NxMSampler</code> must have at least N distinct classes in it.</p>"},{"location":"samplers/#randomnxmsampler","title":"RandomNxMSampler","text":"<p>An implementation of <code>NxMSampler</code> that only samples each individual class once, always sampling M samples per class. Thus, the length of the sampler is equal to the number of distinct classes multiplied by M. If the number of distinct classes is not divisible by N, either the extra classes are omitted if <code>drop_last</code> is True, or additional classes are sampled again otherwise.</p> <p>Each batch in a <code>RandomNxMSampler</code> will have samples from N classes that are distinct. The M samples for an individual class are sampled randomly from all the samples for that class present in the dataset. If there are less samples for a class than M, the samples for that class will be repeated.</p> <p>For example, for a dataset with 10 samples and 3 classes</p> <pre><code>labels = torch.tensor([0, 0, 0, 0, 0, 1, 1, 1, 2, 2])\nsampler = RandomNxMSampler(labels, N_classes_per_batch=2, M_samples_per_class=2, shuffle=False, drop_last=True)\nlist(sampler) # [3, 4, 5, 7]\n</code></pre> <p>the <code>RandomNxMSampler</code> with <code>N=2</code> and <code>M=2</code> and <code>drop_last=True</code> will return 1 batch of 2*2 samples (samples for each class are picked at ramdom) from the first two classes (the last class will be droped).</p> <p>If <code>drop_last=False</code>, the sampler will return 2 batches of 2*2 samples, the second batch having 2 samples from the last class + 2 samples from the first class again.</p> <pre><code>labels = torch.tensor([0, 0, 0, 0, 0, 1, 1, 1, 2, 2])\nsampler = RandomNxMSampler(labels, N_classes_per_batch=2, M_samples_per_class=2, shuffle=False, drop_last=False)\nlist(sampler) # [2, 0, 6, 5, 9, 8, 0, 3]\n</code></pre> <p>Note</p> <p>If <code>shuffle=True</code>, the order the classes are sampled in is random.</p>"},{"location":"samplers/#exhaustivenxmsampler","title":"ExhaustiveNxMSampler","text":"<p>An implementation of <code>NxMSampler</code> that will iterate over elements of the dataset by batches of N * M samples, until either all the dataset has been sampled, or there are less than N * M samples remaining.</p> <p>As much as possible, the batches will have N distinct classes, but this is not guaranteed, for instance if there are only samples from a single class remaining.</p> <p>Samples for a class might be repeated if the number of samples for that class is not divisible by <code>M</code>.</p> <p>For example, for a dataset with 10 samples and 3 classes</p> <pre><code>labels = torch.tensor([0, 0, 0, 0, 0, 1, 1, 1, 2, 2])\nsampler = ExhaustiveNxMSampler(labels, N_classes_per_batch=2, M_samples_per_class=2, shuffle=False)\nlist(sampler) # [0, 1, 5, 6, 8, 9, 2, 3, 7, 5, 4, 0]\n</code></pre> <p>the <code>ExhaustiveNxMSampler</code> with <code>N=2</code> and <code>M=2</code> will return 3 batch of 2*2 samples. Classes <code>0</code> and <code>1</code> have 5 and 3 samples respectively, which is not divisible by <code>M=2</code>, so both these classes will have their first item sampled twice.</p> <ul> <li>The first batch (<code>0, 1, 5, 6</code>) contains the first 2 samples from class <code>0</code> (indices <code>0</code> and <code>1</code>) and the first two samples from class <code>1</code> (indices <code>5</code> and <code>6</code>)</li> <li>The second batch (<code>8, 9, 2, 3</code>) contains the first 2 samples from class <code>2</code> (indices <code>8</code> and <code>9</code>) and two more samples from class <code>0</code> (indices <code>2</code> and <code>3</code>)</li> <li>The last batch (<code>7, 5, 4, 0</code>) contains the last sample from class <code>1</code> (index <code>7</code>) plus an extra sample from the same class, which has already been sampled but is required to have <code>M=2</code> samples per class (index <code>5</code>), and also the last sample from class <code>0</code> (index <code>4</code>) plus another sample from the same class that has already been sampled (index <code>0</code>)</li> </ul> <p>Note</p> <p>If <code>shuffle=True</code>, the order the classes are sampled in is random, and the order of the samples for each class is also random.</p>"},{"location":"samplers/#distributed-sampling","title":"Distributed sampling","text":"<p>The <code>NxMSampler</code> implementations support distributed sampling for training with multiple gpus. The samplers will assign the batches to each replica in turn (the first batch of size N * M goes to the first gpu, the second batch to the second gpu, the third batch to the first, and so on...). To make sure that each replica receives the same amount of data, the first few batches might be repeated (for exemple, if there are 3 batches of size N * M and 2 gpus, the first gpu will receive the first and third batches, while the second gpu will receive the second and first batches).</p> <p>Warning</p> <p>In distributed mode, as with pytorch's <code>DistributedSampler</code>, calling the <code>set_epoch</code> method on the sampler at the beginning of each epoch before creating the <code>DataLoader</code> iterator is necessary to make shuffling work properly, otherwise the same ordering will be always used.</p>"},{"location":"samplers/#cli","title":"CLI","text":"<p>The sampler class used during training of the Binary Attribute Encoder is set in the <code>data_config</code> config parameter. By default, the <code>RandomNxMSampler</code> is used. To use another sampler, set the <code>sampler</code> attribute of the data configuration to the class path of the sampler to use:</p> <p>Example</p> <pre><code>balr train resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv data.sampler=balr.samplers.nxm_samplers.ExhaustiveNxMSampler\n</code></pre>"},{"location":"trainer/","title":"Binary-Attribute Encoder training","text":"<p>The BA-LR toolkit provides a <code>BinaryAttributeEncoderTrainer</code> class to train an AutoEncoder for binary-attribute representation extraction from embedding vectors.</p>"},{"location":"trainer/#binaryattributeencodertrainer","title":"BinaryAttributeEncoderTrainer","text":"<pre><code>class BinaryAttributeEncoderTrainer:\n    def __init__(\n        self,\n        train: AudioDataset,\n        val: AudioDataset,\n        data_config: DataConfig,\n        loss_funcs: list[BaseAutoEncoderLoss],\n        input_dim: int = 256,\n        internal_dim: int = 512,\n        learning_rate: float = 0.001,\n        epochs: int = 100,\n        seed: int = 1234,\n        save_dir: Path | None = None,\n        save_period: int = 0,\n        log_period: int = 2,\n        val_period: int = 10,\n        device: str | torch.device = \"cpu\",\n        **kwargs,\n    ):\n</code></pre> <p>Parameters:</p> <ul> <li>train: the training dataset.</li> <li>val: the validation dataset.</li> <li>data_config: config parameters for the <code>Dataloader</code> used with the datasets.</li> <li>loss_funcs: a list of loss functions to use during training and validation.</li> <li>input_dim: the input dimension for the AutoEncoder (i.e. the embedding dimension).</li> <li>internal_dim: the internal or output dimension for the AutoEncoder (i.e. the dimension of the binary-attribute representation).</li> <li>learning_rate: the learning rate.</li> <li>epochs: the number of epochs for training.</li> <li>seed: the seed for random functions.</li> <li>save_dir: directory where the training output (logs, metrics and model checkpoints) are saved.</li> <li>save_period: save model checkpoint every x epochs.</li> <li>log_period: logs metrics every x epochs.</li> <li>val_period: run validation every x epochs.</li> <li>device: the device to use the model on.</li> </ul>"},{"location":"trainer/#training","title":"Training","text":"<p>Once the <code>BinaryAttributeEncoderTrainer</code> class has been initialized with the proper parameters, the <code>train</code> method will run training on the training dataset for the set number of epochs.</p>"},{"location":"trainer/#cli","title":"CLI","text":"<p>The BA-LR cli provides a <code>train</code> command to train a <code>BinaryAttributeEncoder</code> using the <code>BinaryAttributeEncoderTrainer</code>.</p> <p>Parameters:</p> <ul> <li>train: the path to the training dataset.</li> <li>val: the path to the validation dataset.</li> <li>save_dir: directory where the training output (logs, metrics and model checkpoints) are saved. By default, results will be saved to <code>./runs/trainX</code>, X being incremented as needed (<code>train2</code>, <code>train3</code>, etc. on successive runs).</li> <li>device: the device to use the model on.</li> <li>overrides: optional hydra config overrides.</li> </ul> <p>Warning</p> <p>Both the training and validation datasets must provide embeddings for all their samples. But since only embeddings are needed for training, you can run training on datasets that only provide embeddings (i.e. without audio files).</p> <p>Example</p> <pre><code>balr train resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv\n</code></pre> <p>will train a <code>BinaryAttributeAutoEncoder</code> model on the <code>voxceleb2/train.csv</code> dataset and use the <code>voxceleb2/test.csv</code> dataset for validation. Results will be saved by default to the <code>./runs/train</code> directory. Training will run by default on the <code>cpu</code> device.</p> <p>Example</p> <pre><code>balr train --save-dir training_output --device cuda resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv trainer.epochs=10 'trainer.losses=[mse, arcface]'\n</code></pre> <p>This more complex command will run training on the same datasets, but specifies the device (<code>cuda</code>) and the output directory, as well as modifies the trainer's config for <code>epochs</code> and <code>losses</code> parameters using hydra overrides.</p>"},{"location":"trainer/#distributed-training","title":"Distributed training","text":"<p>It is possible to run training on multiple GPUs with pytorch's torchrun launcher script.</p> <p>Note</p> <p>When using <code>torchrun</code>, you must call the cli's <code>balr/cli/main.py</code> module instead of the <code>balr</code> command.</p> <p>Example</p> <pre><code>torchrun --nproc_per_node 2 balr/cli/main.py train resources/data/voxceleb2/train.csv resources/data/voxceleb2/test.csv --device cuda:0,1\n</code></pre> <p>will run training on two GPUs. The samplers used with the dataloaders must support distributed sampling.</p>"},{"location":"scoring/beta/","title":"Beta-Bernouilli Scoring","text":"<p>In this probabilistic modeling of speakers, each speaker \\(l\\) is characterized by the probability \\(p^i_l \\in [0,1]\\) of activating attribute \\(ba_i\\). \\(p^i_l\\) is a latent variable that is never observed.</p> <p>We define \\(f\\) as the probability density corresponding to the probability \\(p\\) of activating the attribute for a speaker. \\(f\\) can be estimated either from a parametric family or using the empirical distribution.</p> <p>For a Beta-Bernouilli model, \\(f\\) is parameterized by a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\):</p> \\[ f(p \\mid \\alpha, \\beta) = \\frac{p^{\\alpha -1} (1-p)^{\\beta -1}}{B(\\alpha, \\beta)} \\] <p>where \\(B\\) is the Beta function. This distribution is chosen because it is the conjugate prior of the binomial distribution, which simplifies likelihood computations.</p>"},{"location":"scoring/beta/#parameter-estimation","title":"Parameter estimation","text":"<p>The parameters \\(\\alpha\\) and \\(\\beta\\) can be estimated by maximum likelihood from the activation and non-activation statistics of the attributes per speaker. The formulas are given in the work of Thomas Minka. Minka provides the maximum likelihood estimate for Dirichlet multinomial distributions. The Beta-Bernoulli scoring is a special case of a Dirichlet-Multinomial model (with K=2 parameters). The Beta-Bernouilli implementation is done directly using the Dirichlet-Multinomial model.</p>"},{"location":"scoring/beta/#scoring","title":"Scoring","text":"<p>The choice of a Beta distribution for \\(f\\) allows the likelihoods to be computed explicitly.</p> \\[ L(a, n)  = \\int_{p=0}^1  \\binom{a+n}{a} p^a (1-p)^n f(p) dp = \\frac{B(\\alpha+a, \\beta+n)}{B(\\alpha, \\beta)} \\] <p>which allows us to derive the likelihood ratio</p> \\[ LR((a^e, n^e) , (a^t, b^t) ) = \\frac{B(\\alpha+a^e +a^t, \\beta+n^e + n^t) B(\\alpha, \\beta) }{B(\\alpha+a^e, \\beta+n^e) B(\\alpha+a^t, \\beta+n^t)} \\]"},{"location":"scoring/scorers/","title":"Scorers","text":"<p>The BA-LR toolkit provides different scorer classes which implement the scoring models described in the previous section.</p>"},{"location":"scoring/scorers/#speechllrscorer","title":"SpeechLLRScorer","text":"<pre><code>class SpeechLLRScorer(LLRScorer):\n    def __init__(\n        self,\n        n_attributes: int = 512,\n        drop_in: float = 0.12,\n        typicality_threshold: float = 1e-4,\n        checkpoint_path: str | Path | None = None,\n        save_dir: str | Path | None = None,\n        device: str | torch.device = \"cpu\",\n        *args,\n        **kwargs,\n    ) -&gt; None:\n</code></pre> <p>Parameters:</p> <ul> <li>n_attributes: the number of binary attributes.</li> <li>drop_in: the drop-in parameter.</li> <li>typicality_threshold: typicality threshold below which LLR will not be predicted (returns 1).</li> <li>checkpoint_path: path to the saved model weights.</li> <li>save_dir: directory where the model weights are saved.</li> <li>device: the device to use the model on.</li> </ul>"},{"location":"scoring/scorers/#dnallrscorer","title":"DNALLRScorer","text":"<pre><code>class DNALLRScorer(LLRScorer):\n    def __init__(\n        self,\n        n_attributes: int = 512,\n        drop_in: float = 0.12,\n        typicality_threshold: float = 1e-4,\n        checkpoint_path: str | Path | None = None,\n        save_dir: str | Path | None = None,\n        device: str | torch.device = \"cpu\",\n        *args,\n        **kwargs,\n    ) -&gt; None:\n</code></pre> <p>Parameters:</p> <ul> <li>n_attributes: the number of binary attributes.</li> <li>drop_in: the drop-in parameter.</li> <li>typicality_threshold: typicality threshold below which LLR will not be predicted (returns 1).</li> <li>checkpoint_path: path to the saved model weights.</li> <li>save_dir: directory where the model weights are saved.</li> <li>device: the device to use the model on.</li> </ul>"},{"location":"scoring/scorers/#maxllrscorer","title":"MaxLLRScorer","text":"<pre><code>class MaxLLRScorer(Scorer):\n    def __init__(\n        self,\n        n_attributes: int = 512,\n        f: float = 0.5,\n        p: float = 0.9,\n        q: float = 0.1,\n        checkpoint_path: str | Path | None = None,\n        save_dir: str | Path | None = None,\n        device: str | torch.device = \"cpu\",\n    ) -&gt; None:\n</code></pre> <p>Parameters:</p> <ul> <li>n_attributes: the number of binary attributes.</li> <li>f: initial value for EM approximation of \\(f\\).</li> <li>p: initial value for EM approximation of \\(p\\).</li> <li>q: initial value for EM approximation of \\(q\\).</li> <li>checkpoint_path: path to the saved model weights.</li> <li>save_dir: directory where the model weights are saved.</li> <li>device: the device to use the model on.</li> <li>n_iterations: the number of iteration steps during EM approximation of the parameters.</li> </ul>"},{"location":"scoring/scorers/#dirichletmultinomialscorer","title":"DirichletMultinomialScorer","text":"<pre><code>class DirichletMultinomialScorer(Scorer):\n    def __init__(\n        self,\n        K: int = 2,\n        eps: float = 1e-8,\n        n_attributes: int = 512,\n        checkpoint_path: str | Path | None = None,\n        save_dir: str | Path | None = None,\n        device: str | torch.device = \"cpu\",\n        *args,\n        **kwargs,\n    ) -&gt; None:\n</code></pre> <p>Parameters:</p> <ul> <li>K: number of quantized states per attribute (2 for binary).</li> <li>eps: non zero small value for numerical stability.</li> <li>n_attributes: the number of binary attributes.</li> <li>checkpoint_path: path to the saved model weights.</li> <li>save_dir: directory where the model weights are saved.</li> <li>device: the device to use the model on.</li> <li>n_iterations: the number of iteration steps during EM approximation of the parameters.</li> </ul> <p>Note</p> <p>The Beta-Bernoulli scoring is a special case of a Dirichlet-Multinomial model (with K=2 parameters).</p>"},{"location":"scoring/scorers/#usage","title":"Usage","text":"<p>The BA-LR cli provides a <code>score</code> command to train a scorer on a reference dataset and to score trial pairs.</p>"},{"location":"scoring/scorers/#training","title":"Training","text":"<p>The <code>balr score train</code> command is used to train a scorer on a given dataset.</p> <p>Parameters:</p> <ul> <li>train: the path to the training dataset.</li> <li>save_dir: directory where the scorer weights are saved. By default, results will be saved to <code>./runs/scorer/trainX</code>, X being incremented as needed (<code>train2</code>, <code>train3</code>, etc. on successive runs).</li> <li>device: the device to train the model on.</li> <li>overrides: optional hydra config overrides, including the <code>scorer</code> parameter which lets you choose which scoring model to train.</li> </ul> <p>Warning</p> <p>Since scoring works on binary attribute vectors, make sure that all the samples in the dataset have binary attribute vectors.</p> <p>Example</p> <pre><code>balr score train resources/data/voxceleb2/train.csv\n</code></pre> <p>will train the default scorer (<code>Beta-Bernoulli</code>) on the <code>voxceleb2/train.csv</code> dataset. The scorer will estimate its parameters over the reference population. The scorer weights will be saved by default to the <code>./runs/scorer/train</code> directory. Training will run by default on the <code>cpu</code> device.</p> <p>Example</p> <pre><code>balr score train resources/data/voxceleb2/train.csv scorer=maxllr scorer.f=0.1 scorer.p=0.7 scorer.q=0.2 scorer.n_iterations=50\n</code></pre> <p>This more complex command will train a <code>MaxLLRScorer</code> on the <code>voxceleb2/train.csv</code> dataset, but also modifies the initial values for the scorer's parameters (f, p, q) as well as the number of iterations for the EM approximation algorithm.</p>"},{"location":"scoring/scorers/#scoring","title":"Scoring","text":"<p>The <code>balr score test</code> command is used to score trial pairs of recordings using a scoring model.</p> <p>Parameters:</p> <ul> <li>test: the path to the testing dataset.</li> <li>trials: the path to the trials list.</li> <li>sep: separator char for trials list. Defaults to \"\\t\".</li> <li>save_dir: directory where the scores will be saved. By default, results will be saved to <code>./runs/scoreX</code>, X being incremented as needed (<code>score2</code>, <code>score3</code>, etc. on successive runs).</li> <li>device: the device to run the model on.</li> <li>overrides: optional hydra config overrides, including the checkpoint path of the scorer weights fit on a reference dataset.</li> </ul> <p>Warning</p> <p>Most scoring models require to be fitted on a reference population to estimate their distribution parameters (see the training section above). When running scoring, make sure to specify the checkpoint path of the fitted model with the <code>scorer.checkpoint_path=...</code> parameter.</p> <p>The <code>balr score test</code> command requires two arguments:</p> <ol> <li>a test dataset containing binary attributes vectors for all its samples,</li> <li>a csv list of comma separated ids for the enrollment and test pairs to score.</li> </ol> <p>For example, with a dataset metadata file such as</p> <pre><code>id,binary_attributes,speaker\nid07417/00028,id07417/00028_ba.txt,id07417\nid03184/00022,id03184/00022_ba.txt,id03184\nid03184/00053,id03184/00053_ba.txt,id03184\nid04961/00169,id04961/00169_ba.txt,id04961\nid04961/00289,id04961/00289_ba.txt,id04961\nid01184/00133,id01184/00133_ba.txt,id01184\nid06261/00159,id06261/00159_ba.txt,id06261\nid06261/00233,id06261/00233_ba.txt,id06261\nid06261/00190,id06261/00190_ba.txt,id06261\nid07531/00142,id07531/00142_ba.txt,id07531\n</code></pre> <p>which can be loaded with <code>AudioDataset.from_path</code>. A trials csv file such as</p> <pre><code>enrollment  test\nid03184/00022,id03184/00053 id07417/00028\nid06261/00159,id06261/00233,id06261/00190   id04961/00169,id04961/00289\n</code></pre> <p>will score two trials</p> <ol> <li>the first comparing the two recordings of speaker <code>id03184</code> againt a single recording of speaker <code>id07417</code></li> <li>the second comparing the three recordings of speaker <code>id06261</code> againt the two recordings of speaker <code>id04961</code></li> </ol> <p>The <code>balr score test</code> command will save the results in a <code>scores.csv</code> file in the <code>runs/scoreX</code> directory:</p> <pre><code>enrollment  test    scores\nid03184/00022,id03184/00053 id07417/00028   0.9382\nid06261/00159,id06261/00233,id06261/00190   id04961/00169,id04961/00289 0.9817\n</code></pre> <p>Example</p> <pre><code>balr score test resources/data/voxceleb2/metadata.csv resources/data/voxceleb2/trials.csv scorer=cosine\n</code></pre> <p>will score the trial lists in the <code>resources/data/voxceleb2/trials.csv</code> file with a <code>CosineSimilarity</code> scorer (this scorer does not need to be fitted, thus it does not require a checkpoint_path). The scores will be saved by default to the <code>./runs/score</code> directory. Scoring will run by default on the <code>cpu</code> device.</p> <p>Example</p> <pre><code>balr score test resources/data/voxceleb2/metadata.csv resources/data/voxceleb2/trials.csv scorer=beta scorer.checkpoint_path=runs/scorer/train/scorer.pt\n</code></pre> <p>This command will score the trial lists in the <code>resources/data/voxceleb2/trials.csv</code> file with a <code>BetaBernouilli</code> scorer using the scorer weights saved from its training in the <code>runs/scorer/train/scorer.pt</code> file.</p>"},{"location":"scoring/scoring/","title":"Likelihood Ratio Scoring","text":"<p>The BA-LR (Binary Attribute Likelihood Ratio) method introduced in Imen Ben-Amor's thesis consists in replacing typical speaker embeddings by vectors of binary attributes. Theses attributes are supposed to be independent; therefore a speaker identification scoring can be computed based on activation statistics of the attributes over a reference population. One of the hypothesis of the model is that each attribute encodes a specific caracteristic shared among a subset of speakers. It is therefore more than just a binarized version of speaker embeddings.</p> <p>By replacing the usual scoring in the high dimension space of speaker embeddings by a score that can be decomposed according to each binary attribute, the BA-LR method offers increased explicability.</p> <p>We propose two models for LLR scoring of binary attribute speaker embeddings. For the first model, each speaker either possesses or does not possess each attribute. For the second model, each speaker is instead assigned a probability of possessing or not possessing each attribute. The first model is therefore a special case of the second. The second model is more generic and is based on standard statistical methods.</p>"},{"location":"scoring/scoring/#notation","title":"Notation","text":"<p>We write \\(n\\) the number of binary attributes. The activations of the attributes are written as \\(ba_i \\in \\{ 0, 1\\}\\). A recording \\(x\\) is represented by the series of binary attribute activations $ x = [ ba_0, ..., ba_i, ..., ba_n ] $.</p> <p>For a set \\(X\\) of \\(p\\) recordings, we define the number of activations \\(a_i = \\sum_{j=1}^p 1_{ba_i^j = 1}\\) and non activations \\(n_i = \\sum_{j=1}^p 1_{ba_i^j = 0} = p - a_i\\) of the attribute \\(ba_i\\) for the recordings of \\(X\\).</p> <p>To estimate the parameters of the scoring model, we have a training set of \\(k\\) speakers. For each speaker \\(l \\in [1, k]\\), we have a set \\(X_l\\) of \\(p_l\\) recodings.</p> \\[ X_l = [(a_1^l, n_1^l), .., (a_i^l, n_i^l), .., (a_k^l, n_k^k)] \\] <p>During inference for speaker verification, we have a set of enrollment recordings \\(X_e\\) and a set of test recordings \\(X_t\\).</p> \\[ X_e = [(a_1^e, n_1^e), .., (a_i^e, n_i^e), .., (a_n^e, n_n^e)]\\\\ X_t = [(a_1^t, n_1^t), .., (a_i^t, n_i^t), .., (a_n^t, n_n^t)] \\] <p>The hypothesis of independence between the attributes implies that the likelihood ratio can be factorized according to the attributes:</p> \\[ LR(X_e, X_t) = \\prod_{i=1}^n LR_i(X_e, X_t) =  \\prod_{i=1}^n LR_i((a_i^e, n_i^e), (a_i^t, n_i^t)) \\]"},{"location":"scoring/scoring/#reference-llr-scoring","title":"Reference LLR scoring","text":"<p>The reference scorers from Imen Ben-Amor's thesis model the distribution of attributes over a reference population with three parameters: typicality, drop-out and drop-in.</p> <p>The typicality \\(T_i\\) of a binary attribute \\(b_i\\) is the frequency of speaker pairs sharing the attribute in the reference population.</p> <p>The drop-out \\(Dout_i\\) of attribute \\(b_i\\) is defined as the probability of the attribute disappearing from the profile. This can be due either to a failure to detect the attribute when it is actually present in a recording, or to the attribute being genuinely absent from the recording despite being part of the speaker's profile.</p> <p>The drop-in is the probability of encountering noise leading to a false detection of the attribute in a recording. It is considered independent of the attribute.</p> <p>Two formulas for computing the LLR values for the trials are given: DNA and Speech.</p>"},{"location":"scoring/scoring/#parameter-estimation","title":"Parameter estimation","text":"<p>Each speaker either possesses or does not possess each attribute. Thus, for each speaker \\(l\\) and each attribute, we define the variable \\(y^i_l \\in {0, 1}\\) which is equal to 1 if the speaker possesses the attribute and 0 otherwise. \\(y^i_l\\) is a latent variable that is never observed.</p> <p>The model approximates the latent variable \\(y^i_l\\) with the speaker's profile \\(P_l(BA_i)\\).</p> \\[ P_l(BA_i) = 1_{a_i^l &gt; 0} \\] <p>This gives us an estimator of the typicality corresponding to the proportion of speaker pairs who both have a profile equal to 1.</p> \\[ \\hat{T_i} = \\frac{N_c(P(BA_i)=1)}{N_c} \\] <p>where \\(N_c\\) is the number of speaker pairs and \\(N_c(P(BA_i)=1)\\) is the number of speaker pairs who both have a profile equal to 1. If we write \\(K\\) the number of speakers and \\(K_i^1 = \\sum_{l=1}^K 1_{P_l(BA_i) = 1} = \\sum_{j=1}^K 1_{a_i^l &gt; 0}\\) the number of speakers having a profile equal to 1, we have:</p> \\[ \\hat{T_i} = \\frac{K_i^1 (K_i^1 -1)}{K (K - 1)} \\] <p>The dropout estimator \\(\\hat{Dout_i}\\) relies on the the profile \\(P_l(BA_i)\\) also. We estimate the dropout for each speaker having the attribute in its profile \\(\\hat{Dout_i^l}\\) as well as an average over the speakers.</p> \\[ \\hat{Dout_i^l} = \\frac{n_i^l}{a_i^l + n_i^l} \\\\ \\hat{Dout_i} =  \\frac{\\\\sum_{l = 1}^K \\hat{Dout_i^l}  1_{P_l(BA_i) =1}}{K_i^1} \\] <p>The drop-in probability \\(P_{din}\\) is assumed to depend on the typicality of the attribute and on a parameter \\(Din\\) that is independent of the attribute.</p> \\[ P_{din, i} = Din \\times T_i \\] <p>The parameter \\(Din\\) is selected so as to minimize the speaker verification error (measured in terms of Cllr).</p>"},{"location":"scoring/scoring/#dna-scoring","title":"DNA scoring","text":"<p>This scoring is based on the following assumptions:</p> <ul> <li>drop-in and drop-out occur only in test recordings;</li> <li>the profile established from enrollment recordings is accurate;</li> <li>the drop-in probability is represented by \\(P_{din, i} = Din \\times T_i\\), whereas typicality does not affect the probability of no drop-in.</li> </ul> <p>The value of the likelihood ratio depends solely on the enrollment and test profiles. This scoring assumes there are only one enrollment recording and one test recording.</p> \\[ LR_i((a_i^e, n_i^e), (a_i^t, n_i^t)) = LR_i(P_e(BA_i), P_t(BA_i)) \\] <p>Thus,</p> <ul> <li>\\(LR_i(0,1) = \\frac{Dout_i}{T_i}\\)</li> <li>\\(LR_i(0,0) = \\frac{1}{T_i ( 1 - Din + Dout_i)}\\)</li> <li>\\(LR_i(1,1) = \\frac{1}{T_i ( 1 - Dout_i + Din T_i)}\\)</li> <li>\\(LR_i(1,0) = \\frac{Din T_i}{T_i ( Din T_i + 1  - Din)}\\)</li> </ul> <p>We make the cases \\(01\\) and \\(10\\) equal with the following transformation \\(LR_i(0,1) = LR_i(1,0) = \\frac{LR_i(0,1)+ LR_i(1,0)}{2}\\).</p>"},{"location":"scoring/scoring/#speech-scoring","title":"Speech scoring","text":"<p>This scoring is based on the following assumptions:</p> <ul> <li>drop-in and drop-out can occur in both enrollment and test recordings;</li> <li>drop-in and drop-out events in enrollment recordings are independent of those in test recordings;</li> <li>the drop-in probability is represented by \\(P_{din, i} = Din \\times T_i\\), whereas typicality does not affect the probability of no drop-in.</li> </ul> <p>The value of the likelihood ratio still depends solely on the enrollment and test profiles. This scoring assumes there are only one enrollment recording and one test recording.</p> \\[ LR_i((a_i^e, n_i^e), (a_i^t, n_i^t)) = LR_i(P_e(BA_i), P_t(BA_i)) \\] <p>Thus,</p> <ul> <li>$LR_i(0,1) = \\frac{ (1 -Din ) Din T_i + Dout_i (1 - Dout_i) }{ T_i ( (1-Din) Din T_i + Dout_ i (1 - Dout_i)  + 1 + Din T_i Dout_i ) } $</li> <li>$LR_i(0,0) = \\frac{1 + Dout_i^2}{T_i ( 2 Dout_i (1 - Din) + Dout_i^2 + (1 - Din)^2)} $</li> <li>$LR_i(1,1) = \\frac{1 + (Din T_i)^2 }{T_i ( 2 Din T_i (1 - Dout_i) + ( Din T_i)^2 + (1 - Dout_i )^2 )} $</li> <li>$LR_i(1,0) = \\frac{ (1- Din) Din T_i + Dout_i (1 - Dout_i)}{ T_i ( (1-Din) Din T_i + Dout_i (1- Dout_i) + 1 + Din T_i Dout_i)  } $</li> </ul> <p>We observe that \\(LR_i(0,1) = LR_i(1,0)\\).</p>"},{"location":"scoring/scoring/#maximum-likelihood-ratio-scorer","title":"Maximum Likelihood Ratio Scorer","text":"<p>The Maximum Likelihood Ratio scorer has the same objectives as the reference model but is based on standard statistical methods. The MaxLLR scorer also models the distribution of attributes over a reference population with the same three parameters (typicality, drop-out and drop-in), but instead of assuming that each speaker either possesses or does not possess each attribute, it assigns each speaker a probability of possessing or not possessing each attribute.</p> <p>To avoid confusion, the parameters (typicality, drop-out and drop-in) are renamed (\\(f\\), \\(p\\) and \\(q\\)).</p> <p>For an attribute \\(b_i\\), the population frequency \\(f_i\\) is defined as the probability that a speaker possesses the attribute. By definition, \\(f_i = \\mathbb{E}[y^i_l]\\). We denote \\(p_i\\) as the activation frequency of the attribute in a recording for speakers who possess the attribute, and \\(q_i\\) as the activation frequency of the attribute in a recording for speakers who do not possess the attribute.</p>"},{"location":"scoring/scoring/#parameter-estimation_1","title":"Parameter estimation","text":"<p>We propose here an estimation based on the maximum likelihood principle. Since the likelihood expression involves the latent variables \\(y^l_i\\), maximizing the likelihood requires the use of the Expectation-Maximization method.</p> <p>For a speaker \\(l\\), the likelihood of the attribute activations conditioned on \\(y^l\\) is given by:</p> \\[ \\mathbb{L}((a^{l}, n^{l}) \\mid y^l) = p^a (1-p)^n \\cdot \\mathbf{1}_{y^l = 1} +  q^a (1-q)^n \\cdot \\mathbf{1}_{y^l = 0} \\] <p>We denote \\(\\theta = (f, p, q)\\). The update of \\(\\theta\\) is given by:</p> \\[ \\theta_{m+1} = \\arg\\max_{\\theta} \\sum_{l=1}^k \\mathbb{E}_{y^l \\mid (a^l, n^l), \\theta_m}\\left[ \\log\\left( \\mathbb{L}((a^l, n^l), y^l \\mid \\theta_m) \\right) \\right] \\] <p>We denote \\(\\tilde{f}^l = \\mathbb{P}(y^l = 1 \\mid (a^l, n^l), \\theta_m)\\), the posterior probability that speaker \\(l\\) possesses the attribute. Thus, we have:</p> \\[ \\theta_{m+1} = \\arg\\max_{\\theta} \\sum_{l=1}^k \\left[ \\tilde{f}^l \\log( \\mathbb{P}((a^l , n^l), y^l = 1 \\mid \\theta_m)) + (1-\\tilde{f}^l) \\log( \\mathbb{P}((a^l , n^l), y^l = 0 \\mid \\theta_m)) \\right] \\] <p>We can determine \\(\\tilde{f}^l\\):</p> \\[ \\tilde{f}^l = \\mathbb{P}(y^l=1 \\mid (a^l , n^l), \\theta_m) = \\frac{\\mathbb{P}((a^l,n^l), y^l=1 \\mid \\theta_m)}{ \\mathbb{P}( (a^l, n^l) \\mid \\theta_m)} \\] \\[ \\tilde{f}^l = \\frac{ f \\cdot \\mathbb{P}((a^l, n^l) \\mid y^l=1, \\theta_m) }{ f \\cdot \\mathbb{P}((a^l, n^l) \\mid y^l=1, \\theta_m) + (1-f) \\cdot \\mathbb{P}((a^l, n^l) \\mid y^l=0, \\theta_m) } \\] <p>By substituting \\(\\tilde{f}^l\\) into the expression for the log-likelihood and differentiating with respect to the parameters \\(f, p\\), and \\(q\\), we obtain the parameter update formulas:</p> \\[ \\hat{f} = \\frac{\\sum_l \\tilde{f}^l}{k} \\] \\[ \\hat{p} = \\frac{\\sum_l a^l \\tilde{f}^l}{\\sum_l (a^l + n^l) \\tilde{f}^l} \\] \\[ \\hat{q} = \\frac{\\sum_l a^l (1-\\tilde{f}^l)}{\\sum_l (a^l + n^l)(1- \\tilde{f}^l)} \\] <p>The initialization of the EM algorithm can be crucial. For now, we naively initialize the algorithm with \\(f_0 = 0.5\\), \\(p_0 = 0.9\\), and \\(q_0 = 0.1\\). Indeed, the interpretation of \\(f\\) implies that \\(p &gt; q\\).</p>"},{"location":"scoring/scoring/#scoring","title":"Scoring","text":"<p>We define the likelihood of the observations based on the model parameters:</p> \\[ p((a, n) | f, p, q) = f p^a (1-p)^n + (1-f) q^a (1-q)^n \\] <p>This allows us to define the likelihood ratio:</p> \\[ LR((a^e, n^e), (a^t, n^t)) = \\frac{p((a^e + a^t, n^e + n^t) | f, p, q)}{p((a^e, n^e) | f, p, q) \\cdot p((a^t, n^t) | f, p, q)} \\] <p>When \\(n^e + a^e = n^t + a^t = 1\\), we have the following simplified formulas:</p> <ul> <li>\\(LR(0,0) = LR((0,1),(0,1)) = \\frac{f (1-p)^2 + (1-f) (1-q)^2}{(f (1-p) + (1-f) (1-q))^2}\\)</li> <li>\\(LR(0,1) = LR(1,0) = LR((0,1),(1,0)) = \\frac{f p (1-p) + (1-f) q (1-q)}{(f (1-p) + (1-f)(1-q)) \\cdot (f p + (1-f) q)}\\)</li> <li>\\(LR(1,1) = LR((1,0),(1,0)) = \\frac{f p^2 + (1-f) q^2}{(f p + (1-f) q)^2}\\)</li> </ul>"}]}